#!/usr/bin/env -S uv run
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "jsonschema>=4.0",
# ]
# ///
"""
ä»£ç†äººåˆ†æ´¾æ™ºæ…§åˆ†æå·¥å…·

å¾ç³¾æ­£æ­·å²å’Œè­¦å‘Šè¨˜éŒ„ä¸­è­˜åˆ¥æ¨¡å¼ã€åˆ†ææ ¹å› ã€æä¾›æ”¹é€²å»ºè­°ã€è¿½è¹¤è¶¨å‹¢ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
  python agent_dispatch_analytics.py analyze    - åˆ†æç³¾æ­£æ­·å²æ¨¡å¼
  python agent_dispatch_analytics.py suggest    - ç”Ÿæˆæ”¹é€²å»ºè­°
  python agent_dispatch_analytics.py trends     - è¿½è¹¤èª¤åˆ¤ç‡è¶¨å‹¢
  python agent_dispatch_analytics.py report     - ç”Ÿæˆå®Œæ•´åˆ†æå ±å‘Š

ç‰ˆæœ¬ï¼šv0.12.N.11
ä½œè€…ï¼šbasil-hook-architect
æ—¥æœŸï¼š2025-10-18
"""

import json
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict, Counter
import statistics


# ===== é…ç½® =====

PROJECT_ROOT = Path(".").resolve()
CORRECTION_LOG_FILE = PROJECT_ROOT / ".claude/hook-logs/agent-dispatch-corrections.jsonl"
WARNINGS_LOG_FILE = PROJECT_ROOT / ".claude/hook-logs/agent-dispatch-warnings.jsonl"
REPORT_FILE = PROJECT_ROOT / ".claude/hook-logs/agent-dispatch-analysis-report.md"


# ===== æ•¸æ“šæ¨¡å‹ =====

class CorrectionRecord:
    """ç³¾æ­£è¨˜éŒ„"""
    def __init__(self, data: Dict[str, Any]):
        self.timestamp = data.get("timestamp")
        self.task_type = data.get("task_type", "æœªçŸ¥")
        self.wrong_agent = data.get("wrong_agent", "æœªçŸ¥")
        self.correct_agent = data.get("correct_agent", "æœªçŸ¥")
        self.prompt_preview = data.get("prompt_preview", "")
        self.metadata = data.get("metadata", {})
        self.actual_task_type = self.metadata.get("actual_task_type")
        self.detected_task_type = self.metadata.get("detected_task_type")
        self.reason = self.metadata.get("reason")

    @property
    def is_misdetection(self) -> bool:
        """æ˜¯å¦æ˜¯ä»»å‹™é¡å‹èª¤åˆ¤ï¼ˆactual != detectedï¼‰"""
        if self.actual_task_type is None or self.detected_task_type is None:
            return False
        return self.actual_task_type != self.detected_task_type


class WarningRecord:
    """è­¦å‘Šè¨˜éŒ„"""
    def __init__(self, data: Dict[str, Any]):
        self.timestamp = data.get("timestamp")
        self.warning_type = data.get("warning_type", "æœªçŸ¥")
        self.severity = data.get("severity", "medium")  # low, medium, high
        self.prompt_preview = data.get("prompt_preview", "")
        self.reason = data.get("reason", "")
        self.suggestion = data.get("suggestion", "")
        self.metadata = data.get("metadata", {})


# ===== æ•¸æ“šè®€å– =====

def read_corrections(limit: Optional[int] = None) -> List[CorrectionRecord]:
    """è®€å–ç³¾æ­£æ­·å²è¨˜éŒ„"""
    if not CORRECTION_LOG_FILE.exists():
        return []

    records = []
    with open(CORRECTION_LOG_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                records.append(CorrectionRecord(data))
            except json.JSONDecodeError:
                continue

    if limit:
        return records[-limit:]
    return records


def read_warnings(limit: Optional[int] = None) -> List[WarningRecord]:
    """è®€å–è­¦å‘Šè¨˜éŒ„"""
    if not WARNINGS_LOG_FILE.exists():
        return []

    records = []
    with open(WARNINGS_LOG_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                records.append(WarningRecord(data))
            except json.JSONDecodeError:
                continue

    if limit:
        return records[-limit:]
    return records


# ===== æ¨¡å¼è­˜åˆ¥æ¨¡çµ„ =====

class PatternAnalyzer:
    """æ¨¡å¼è­˜åˆ¥åˆ†æ"""

    def __init__(self, corrections: List[CorrectionRecord], warnings: List[WarningRecord]):
        self.corrections = corrections
        self.warnings = warnings

    def analyze_correction_patterns(self, limit: int = 100) -> Dict[str, Any]:
        """åˆ†ææœ€è¿‘ N ç­†ç³¾æ­£è¨˜éŒ„çš„æ¨¡å¼"""
        recent = self.corrections[-limit:] if len(self.corrections) > limit else self.corrections

        if not recent:
            return {
                "total_corrections": 0,
                "task_type_distribution": {},
                "agent_confusion_matrix": {},
                "misdetection_rate": 0,
                "most_confused_agents": [],
                "common_keywords_in_errors": [],
            }

        # 1. ä»»å‹™é¡å‹åˆ†ä½ˆ
        task_type_counter = Counter(r.task_type for r in recent)

        # 2. ä»£ç†äººæ··æ·†çŸ©é™£
        agent_confusion = defaultdict(lambda: defaultdict(int))
        for r in recent:
            agent_confusion[r.wrong_agent][r.correct_agent] += 1

        # 3. èª¤åˆ¤ç‡
        misdetections = [r for r in recent if r.is_misdetection]
        misdetection_rate = len(misdetections) / len(recent) if recent else 0

        # 4. æœ€å®¹æ˜“æ··æ·†çš„ä»£ç†äººå°
        agent_pairs = []
        for wrong, corrects in agent_confusion.items():
            for correct, count in corrects.items():
                agent_pairs.append((wrong, correct, count))
        agent_pairs.sort(key=lambda x: -x[2])

        # 5. å¸¸è¦‹çš„éŒ¯èª¤é—œéµå­—
        keywords = defaultdict(int)
        for r in recent:
            if r.reason:
                # æå–ã€ŒHookã€ã€ŒPhaseã€ç­‰é—œéµå­—
                for keyword in re.findall(r"[A-Za-z\u4e00-\u9fff]+", r.reason):
                    if len(keyword) > 1:
                        keywords[keyword] += 1

        top_keywords = sorted(keywords.items(), key=lambda x: -x[1])[:10]

        return {
            "total_corrections": len(recent),
            "task_type_distribution": dict(task_type_counter),
            "agent_confusion_matrix": {
                wrong: dict(corrects)
                for wrong, corrects in agent_confusion.items()
            },
            "misdetection_rate": round(misdetection_rate * 100, 2),
            "most_confused_agent_pairs": agent_pairs[:5],
            "common_error_reasons": [
                {
                    "detected_type": r.detected_task_type,
                    "actual_type": r.actual_task_type,
                    "reason": r.reason,
                    "count": sum(1 for x in misdetections
                               if x.detected_task_type == r.detected_task_type
                               and x.actual_task_type == r.actual_task_type)
                }
                for r in misdetections
            ][:5],
        }

    def analyze_warning_patterns(self) -> Dict[str, Any]:
        """åˆ†æè­¦å‘Šè¨˜éŒ„æ¨¡å¼"""
        if not self.warnings:
            return {
                "total_warnings": 0,
                "by_severity": {},
                "by_type": {},
                "high_severity_warnings": [],
            }

        severity_counter = Counter(w.severity for w in self.warnings)
        warning_type_counter = Counter(w.warning_type for w in self.warnings)

        # é«˜å„ªå…ˆç´šè­¦å‘Š
        high_warnings = [w for w in self.warnings if w.severity == "high"]
        high_warnings_summary = [
            {
                "type": w.warning_type,
                "reason": w.reason,
                "suggestion": w.suggestion,
                "timestamp": w.timestamp,
            }
            for w in high_warnings[-5:]
        ]

        return {
            "total_warnings": len(self.warnings),
            "by_severity": dict(severity_counter),
            "by_type": dict(warning_type_counter),
            "high_severity_warnings": high_warnings_summary,
        }


# ===== æ ¹å› åˆ†ææ¨¡çµ„ =====

class RootCauseAnalyzer:
    """æ ¹å› åˆ†æ"""

    def __init__(self, corrections: List[CorrectionRecord], warnings: List[WarningRecord]):
        self.corrections = corrections
        self.warnings = warnings

    def analyze_root_causes(self) -> Dict[str, Any]:
        """åˆ†æèª¤åˆ¤çš„æ ¹æœ¬åŸå› """
        misdetections = [r for r in self.corrections if r.is_misdetection]

        if not misdetections:
            return {
                "misdetection_count": 0,
                "root_causes": [],
                "affected_task_types": [],
                "affected_agents": [],
            }

        # 1. æ ¹å› åˆ†çµ„
        cause_groups = defaultdict(list)
        for record in misdetections:
            cause_groups[record.reason].append(record)

        # 2. ä»»å‹™é¡å‹åˆ†æ
        actual_task_types = Counter(r.actual_task_type for r in misdetections)
        detected_task_types = Counter(r.detected_task_type for r in misdetections)

        # 3. ç›¸é—œä»£ç†äºº
        affected_agents_set = set()
        for r in misdetections:
            affected_agents_set.add(r.wrong_agent)
            affected_agents_set.add(r.correct_agent)

        root_causes = [
            {
                "cause": cause,
                "frequency": len(records),
                "examples": [
                    {
                        "actual_type": r.actual_task_type,
                        "detected_type": r.detected_task_type,
                        "wrong_agent": r.wrong_agent,
                        "correct_agent": r.correct_agent,
                        "prompt_preview": r.prompt_preview[:100],
                    }
                    for r in records[:2]
                ],
            }
            for cause, records in sorted(
                cause_groups.items(),
                key=lambda x: -len(x[1])
            )
        ]

        return {
            "misdetection_count": len(misdetections),
            "root_causes": root_causes,
            "affected_actual_task_types": dict(actual_task_types),
            "affected_detected_task_types": dict(detected_task_types),
            "affected_agents": sorted(list(affected_agents_set)),
        }

    def analyze_keyword_conflicts(self) -> Dict[str, Any]:
        """åˆ†æé—œéµå­—è¡çª"""
        conflicts = defaultdict(lambda: {
            "wrong_detections": [],
            "correct_tasks": [],
            "conflict_rate": 0,
        })

        # æ”¶é›†æ‰€æœ‰å«æœ‰ç›¸åŒé—œéµå­—çš„èª¤åˆ¤
        for record in self.corrections:
            if record.is_misdetection:
                # æå–é—œéµå­—
                keywords = set()
                for keyword in re.findall(r"Phase [0-9a-zA-Z]+|Hook|æ–‡ä»¶|é‡æ§‹|æ¸¬è©¦", record.prompt_preview):
                    keywords.add(keyword)

                for kw in keywords:
                    conflicts[kw]["wrong_detections"].append({
                        "detected_as": record.detected_task_type,
                        "actually": record.actual_task_type,
                    })

        return {
            "keyword_conflict_summary": {
                kw: {
                    "conflict_count": len(data["wrong_detections"]),
                    "example_conflicts": data["wrong_detections"][:2],
                }
                for kw, data in conflicts.items()
                if data["wrong_detections"]
            }
        }


# ===== æ”¹é€²å»ºè­°æ¨¡çµ„ =====

class ImprovementSuggester:
    """æ”¹é€²å»ºè­°ç”Ÿæˆ"""

    def __init__(self, pattern_analysis: Dict, root_cause_analysis: Dict):
        self.pattern_analysis = pattern_analysis
        self.root_cause_analysis = root_cause_analysis

    def generate_suggestions(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        suggestions = []

        # 1. åŸºæ–¼æœ€å¸¸è¦‹çš„æ··æ·†å°
        if self.pattern_analysis.get("most_confused_agent_pairs"):
            for wrong, correct, count in self.pattern_analysis["most_confused_agent_pairs"][:3]:
                suggestions.append({
                    "category": "ä»£ç†äººåˆ†æ´¾å„ªåŒ–",
                    "priority": "high",
                    "issue": f"{wrong} ç¶“å¸¸è¢«èª¤åˆ¤ç‚º {correct}ï¼ˆ{count} æ¬¡ï¼‰",
                    "suggestion": f"æª¢æŸ¥ {wrong} çš„ä»»å‹™é¡å‹æª¢æ¸¬é‚è¼¯ï¼ŒåŠ å¼·èˆ‡ {correct} çš„å€åˆ†",
                    "impact": "æ¸›å°‘é‡è©¦æ¬¡æ•¸ï¼Œæå‡åˆ†æ´¾æ•ˆç‡",
                })

        # 2. åŸºæ–¼é—œéµå­—è¡çª
        if self.root_cause_analysis.get("affected_detected_task_types"):
            top_misdetected = sorted(
                self.root_cause_analysis["affected_detected_task_types"].items(),
                key=lambda x: -x[1]
            )
            if top_misdetected:
                wrong_type, count = top_misdetected[0]
                suggestions.append({
                    "category": "é—œéµå­—æª¢æ¸¬æ”¹é€²",
                    "priority": "high",
                    "issue": f"ã€Œ{wrong_type}ã€æ˜¯æœ€å¸¸è¦‹çš„èª¤åˆ¤é¡å‹ï¼ˆ{count} æ¬¡èª¤åˆ¤ï¼‰",
                    "suggestion": "æª¢æŸ¥è©²ä»»å‹™é¡å‹çš„é—œéµå­—åŒ¹é…é‚è¼¯ï¼Œé¿å…èˆ‡å…¶ä»–é¡å‹æ··æ·†",
                    "impact": "ç›´æ¥é™ä½èª¤åˆ¤ç‡",
                })

        # 3. åŸºæ–¼æ ¹å› 
        if self.root_cause_analysis.get("root_causes"):
            for cause in self.root_cause_analysis["root_causes"][:2]:
                suggestions.append({
                    "category": "æª¢æ¸¬è¦å‰‡æ”¹é€²",
                    "priority": "medium",
                    "issue": f"æ ¹å› ï¼š{cause['cause']}ï¼ˆå‡ºç¾ {cause['frequency']} æ¬¡ï¼‰",
                    "suggestion": f"èª¿æ•´ç›¸é—œæª¢æ¸¬è¦å‰‡ä»¥é¿å…æ­¤æ ¹å› ",
                    "impact": f"å¯æ¸›å°‘ {cause['frequency']} æ¬¡èª¤åˆ¤",
                })

        # 4. åŸºæ–¼èª¤åˆ¤ç‡
        misdetection_rate = self.pattern_analysis.get("misdetection_rate", 0)
        if misdetection_rate > 20:
            suggestions.append({
                "category": "æ•´é«”ç­–ç•¥èª¿æ•´",
                "priority": "high",
                "issue": f"èª¤åˆ¤ç‡è¼ƒé«˜ï¼ˆ{misdetection_rate}%ï¼‰",
                "suggestion": "è€ƒæ…®æ¡ç”¨åˆ†å±¤æª¢æ¸¬ç­–ç•¥æˆ–åŠ å¼·è¨“ç·´æ•¸æ“š",
                "impact": "é¡¯è‘—æå‡åˆ†æ´¾æº–ç¢ºç‡",
            })

        return {
            "total_suggestions": len(suggestions),
            "suggestions": sorted(suggestions, key=lambda x: (
                {"high": 0, "medium": 1, "low": 2}.get(x["priority"], 3),
                -len(x["issue"])
            )),
        }


# ===== è¶¨å‹¢è¿½è¹¤æ¨¡çµ„ =====

class TrendTracker:
    """è¶¨å‹¢è¿½è¹¤"""

    def __init__(self, corrections: List[CorrectionRecord]):
        self.corrections = corrections

    def track_error_trends(self) -> Dict[str, Any]:
        """è¿½è¹¤èª¤åˆ¤ç‡è¶¨å‹¢"""
        if not self.corrections:
            return {
                "trend_data": [],
                "average_error_rate": 0,
                "trend_direction": "ç©©å®š",
                "prediction": "ç„¡æ³•é æ¸¬",
            }

        # æŒ‰æ™‚é–“åˆ†çµ„
        time_groups = defaultdict(lambda: {"total": 0, "misdetections": 0})

        for record in self.corrections:
            try:
                date = record.timestamp[:10]  # YYYY-MM-DD
                time_groups[date]["total"] += 1
                if record.is_misdetection:
                    time_groups[date]["misdetections"] += 1
            except (TypeError, AttributeError):
                continue

        # æ’åºä¸¦è¨ˆç®—èª¤åˆ¤ç‡
        sorted_dates = sorted(time_groups.keys())
        trend_data = [
            {
                "date": date,
                "total": time_groups[date]["total"],
                "misdetections": time_groups[date]["misdetections"],
                "error_rate": round(
                    (time_groups[date]["misdetections"] / time_groups[date]["total"] * 100)
                    if time_groups[date]["total"] > 0 else 0,
                    2
                ),
            }
            for date in sorted_dates
        ]

        # è¨ˆç®—å¹³å‡èª¤åˆ¤ç‡
        error_rates = [d["error_rate"] for d in trend_data]
        avg_error_rate = statistics.mean(error_rates) if error_rates else 0

        # è¶¨å‹¢æ–¹å‘
        if len(error_rates) >= 2:
            recent_avg = statistics.mean(error_rates[-3:])
            older_avg = statistics.mean(error_rates[:-3]) if len(error_rates) > 3 else error_rates[0]
            if recent_avg < older_avg * 0.8:
                trend_direction = "æ”¹å–„ä¸­ â†“"
            elif recent_avg > older_avg * 1.2:
                trend_direction = "æƒ¡åŒ–ä¸­ â†‘"
            else:
                trend_direction = "ç©©å®š"
        else:
            trend_direction = "æ•¸æ“šä¸è¶³"

        # ç°¡å–®é æ¸¬
        if len(error_rates) >= 3:
            diffs = [error_rates[i+1] - error_rates[i] for i in range(len(error_rates)-1)]
            avg_diff = statistics.mean(diffs)
            if avg_diff < -1:
                prediction = "é æœŸæœƒæŒçºŒæ”¹å–„"
            elif avg_diff > 1:
                prediction = "é æœŸæœƒæŒçºŒæƒ¡åŒ–ï¼Œéœ€è¦æ¡å–è¡Œå‹•"
            else:
                prediction = "é æœŸä¿æŒç©©å®š"
        else:
            prediction = "æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é æ¸¬"

        return {
            "trend_data": trend_data,
            "average_error_rate": round(avg_error_rate, 2),
            "trend_direction": trend_direction,
            "prediction": prediction,
            "data_points": len(trend_data),
        }


# ===== å ±å‘Šç”Ÿæˆ =====

def generate_report(
    pattern_analysis: Dict,
    root_cause_analysis: Dict,
    keyword_analysis: Dict,
    suggestions: Dict,
    trends: Dict,
) -> str:
    """ç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Š"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    report = f"""# ä»£ç†äººåˆ†æ´¾æ™ºæ…§åˆ†æå ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}

---

## ğŸ“Š ç¸½é«”çµ±è¨ˆ

### åŸºæœ¬æŒ‡æ¨™
- **ç¸½ç³¾æ­£æ¬¡æ•¸**: {pattern_analysis.get("total_corrections", 0)}
- **ä»»å‹™é¡å‹èª¤åˆ¤ç‡**: {root_cause_analysis.get("misdetection_count", 0)} æ¬¡
- **å¹³å‡èª¤åˆ¤ç‡**: {trends.get("average_error_rate", 0)}%
- **è¶¨å‹¢**: {trends.get("trend_direction", "æœªçŸ¥")}

### ä»»å‹™é¡å‹åˆ†ä½ˆ
"""

    if pattern_analysis.get("task_type_distribution"):
        for task_type, count in sorted(
            pattern_analysis["task_type_distribution"].items(),
            key=lambda x: -x[1]
        ):
            report += f"- {task_type}: {count} æ¬¡\n"

    report += f"""
---

## ğŸ” å¸¸è¦‹èª¤åˆ¤æ¨¡å¼

### ä»£ç†äººæ··æ·†çŸ©é™£

æœ€å®¹æ˜“æ··æ·†çš„ä»£ç†äººå°ï¼š

"""

    if pattern_analysis.get("most_confused_agent_pairs"):
        for wrong, correct, count in pattern_analysis["most_confused_agent_pairs"]:
            report += f"- {wrong} â†’ {correct}: {count} æ¬¡\n"

    report += f"""
### èª¤åˆ¤åŸå› 

"""

    if root_cause_analysis.get("root_causes"):
        for cause_info in root_cause_analysis["root_causes"][:3]:
            report += f"""#### {cause_info['cause']}

**å‡ºç¾é »ç‡**: {cause_info['frequency']} æ¬¡

**ç¯„ä¾‹**:
"""
            for example in cause_info["examples"]:
                report += f"""- å¯¦éš›ä»»å‹™: {example['actual_type']}
  - èª¤åˆ¤ç‚º: {example['detected_type']}
  - ç›¸é—œä»£ç†äºº: {example['wrong_agent']} â†’ {example['correct_agent']}
  - ä»»å‹™æè¿°: {example['prompt_preview']}...

"""

    report += """---

## ğŸ’¡ é—œéµå­—è¡çªåˆ†æ

"""

    if keyword_analysis.get("keyword_conflict_summary"):
        for keyword, data in keyword_analysis["keyword_conflict_summary"].items():
            report += f"""### é—œéµå­—ã€Œ{keyword}ã€

**è¡çªæ¬¡æ•¸**: {data['conflict_count']}

**ç¯„ä¾‹è¡çª**:
"""
            for conflict in data["example_conflicts"]:
                report += f"- èª¤åˆ¤ç‚º: {conflict['detected_as']}, å¯¦éš›æ˜¯: {conflict['actually']}\n"

    report += """
---

## ğŸ’¡ æ”¹é€²å»ºè­°

"""

    if suggestions.get("suggestions"):
        for i, suggestion in enumerate(suggestions["suggestions"], 1):
            priority_emoji = {
                "high": "ğŸ”´",
                "medium": "ğŸŸ¡",
                "low": "ğŸŸ¢",
            }.get(suggestion["priority"], "âšª")

            report += f"""### {i}. {suggestion['category']} {priority_emoji}

**å•é¡Œ**: {suggestion['issue']}

**å»ºè­°**: {suggestion['suggestion']}

**é æœŸå½±éŸ¿**: {suggestion['impact']}

"""

    report += f"""---

## ğŸ“ˆ è¶¨å‹¢è¿½è¹¤

### èª¤åˆ¤ç‡è®ŠåŒ–è¶¨å‹¢

"""

    if trends.get("trend_data"):
        report += "| æ—¥æœŸ | ç¸½æ•¸ | èª¤åˆ¤ | èª¤åˆ¤ç‡ |\n"
        report += "|------|------|------|--------|\n"
        for data in trends["trend_data"]:
            report += f"| {data['date']} | {data['total']} | {data['misdetections']} | {data['error_rate']}% |\n"

    report += f"""
### è¶¨å‹¢åˆ†æ

- **å¹³å‡èª¤åˆ¤ç‡**: {trends.get("average_error_rate", 0)}%
- **è¶¨å‹¢æ–¹å‘**: {trends.get("trend_direction", "æœªçŸ¥")}
- **é æ¸¬**: {trends.get("prediction", "ç„¡æ³•é æ¸¬")}
- **æ•¸æ“šé»**: {trends.get("data_points", 0)} å¤©

---

## ğŸ¯ å¾ŒçºŒè¡Œå‹•è¨ˆç•«

### ç¬¬ 1 å„ªå…ˆç´šï¼ˆç«‹å³åŸ·è¡Œï¼‰

1. å„ªå…ˆå¯¦æ–½é«˜å„ªå…ˆç´šå»ºè­°
2. é—œæ³¨ã€Œ{pattern_analysis.get('most_confused_agent_pairs', [('', '', 0)])[0][0] if pattern_analysis.get('most_confused_agent_pairs') else 'ä»£ç†äºº'}ã€çš„åˆ†æ´¾é‚è¼¯

### ç¬¬ 2 å„ªå…ˆç´šï¼ˆæœ¬é€±å®Œæˆï¼‰

1. æ”¹é€²é—œéµå­—æª¢æ¸¬æ©Ÿåˆ¶
2. åŠ å¼·æ¸¬è©¦è¦†è“‹ç‡

### ç¬¬ 3 å„ªå…ˆç´šï¼ˆæŒçºŒæ”¹é€²ï¼‰

1. å®šæœŸè¿½è¹¤èª¤åˆ¤ç‡è¶¨å‹¢
2. æ”¶é›†ç”¨æˆ¶åé¥‹æ”¹é€²åˆ†æ´¾è¦å‰‡

---

## ğŸ“ æŠ€è¡“èªªæ˜

**åˆ†æå·¥å…·**: agent_dispatch_analytics.py (v0.12.N.11)

**æ•¸æ“šä¾†æº**:
- ç³¾æ­£æ­·å²: `.claude/hook-logs/agent-dispatch-corrections.jsonl`
- è­¦å‘Šè¨˜éŒ„: `.claude/hook-logs/agent-dispatch-warnings.jsonl`

**åˆ†ææ–¹æ³•**:
- æ¨¡å¼è­˜åˆ¥: çµ±è¨ˆåˆ†æèª¤åˆ¤æ¨¡å¼
- æ ¹å› åˆ†æ: æå–å…±åŒçš„æ ¹æœ¬åŸå› 
- å»ºè­°ç”Ÿæˆ: åŸºæ–¼æ•¸æ“šçš„å¯æ“ä½œå»ºè­°
- è¶¨å‹¢è¿½è¹¤: æ™‚åºæ•¸æ“šåˆ†æ

---

**å ±å‘Šå®Œæˆ**
"""

    return report


# ===== CLI å·¥å…· =====

def cmd_analyze():
    """åˆ†æç³¾æ­£æ­·å²"""
    corrections = read_corrections(limit=100)
    warnings = read_warnings()

    analyzer = PatternAnalyzer(corrections, warnings)
    patterns = analyzer.analyze_correction_patterns()
    warnings_patterns = analyzer.analyze_warning_patterns()

    print("\nğŸ“Š ä»£ç†äººåˆ†æ´¾æ¨¡å¼åˆ†æ\n")
    print(f"ç¸½ç³¾æ­£æ¬¡æ•¸: {patterns['total_corrections']}\n")

    print("ä»»å‹™é¡å‹åˆ†ä½ˆ:")
    for task_type, count in sorted(patterns['task_type_distribution'].items(), key=lambda x: -x[1]):
        print(f"  {task_type}: {count}")

    print("\næœ€å®¹æ˜“æ··æ·†çš„ä»£ç†äººå°:")
    for wrong, correct, count in patterns['most_confused_agent_pairs']:
        print(f"  {wrong} â†’ {correct}: {count} æ¬¡")

    print(f"\nèª¤åˆ¤ç‡: {patterns['misdetection_rate']}%")

    if warnings_patterns['total_warnings'] > 0:
        print(f"\nè­¦å‘Šè¨˜éŒ„: {warnings_patterns['total_warnings']}")


def cmd_suggest():
    """ç”Ÿæˆæ”¹é€²å»ºè­°"""
    corrections = read_corrections(limit=100)
    warnings = read_warnings()

    analyzer = PatternAnalyzer(corrections, warnings)
    patterns = analyzer.analyze_correction_patterns()
    root_causes = RootCauseAnalyzer(corrections, warnings).analyze_root_causes()

    suggester = ImprovementSuggester(patterns, root_causes)
    suggestions = suggester.generate_suggestions()

    print("\nğŸ’¡ æ”¹é€²å»ºè­°\n")
    print(f"ç¸½å»ºè­°æ•¸: {suggestions['total_suggestions']}\n")

    for i, suggestion in enumerate(suggestions['suggestions'], 1):
        print(f"{i}. [{suggestion['priority'].upper()}] {suggestion['category']}")
        print(f"   å•é¡Œ: {suggestion['issue']}")
        print(f"   å»ºè­°: {suggestion['suggestion']}")
        print(f"   å½±éŸ¿: {suggestion['impact']}")
        print()


def cmd_trends():
    """è¿½è¹¤è¶¨å‹¢"""
    corrections = read_corrections()
    tracker = TrendTracker(corrections)
    trends = tracker.track_error_trends()

    print("\nğŸ“ˆ èª¤åˆ¤ç‡è¶¨å‹¢\n")
    print(f"å¹³å‡èª¤åˆ¤ç‡: {trends['average_error_rate']}%")
    print(f"è¶¨å‹¢: {trends['trend_direction']}")
    print(f"é æ¸¬: {trends['prediction']}\n")

    print("æœ€è¿‘ 10 å¤©èª¤åˆ¤ç‡:")
    for data in trends['trend_data'][-10:]:
        bar = "â–ˆ" * int(data['error_rate'] / 5) + "â–‘" * (20 - int(data['error_rate'] / 5))
        print(f"  {data['date']}: {bar} {data['error_rate']}%")


def cmd_report():
    """ç”Ÿæˆå®Œæ•´å ±å‘Š"""
    corrections = read_corrections(limit=100)
    warnings = read_warnings()

    # é€²è¡Œå„ç¨®åˆ†æ
    analyzer = PatternAnalyzer(corrections, warnings)
    patterns = analyzer.analyze_correction_patterns()
    keyword_analysis = analyzer.analyze_warning_patterns()

    root_cause_analyzer = RootCauseAnalyzer(corrections, warnings)
    root_causes = root_cause_analyzer.analyze_root_causes()
    keyword_conflicts = root_cause_analyzer.analyze_keyword_conflicts()

    suggester = ImprovementSuggester(patterns, root_causes)
    suggestions = suggester.generate_suggestions()

    tracker = TrendTracker(corrections)
    trends = tracker.track_error_trends()

    # ç”Ÿæˆå ±å‘Š
    report = generate_report(
        patterns,
        root_causes,
        keyword_conflicts,
        suggestions,
        trends,
    )

    # ä¿å­˜å ±å‘Š
    REPORT_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"\nâœ… å ±å‘Šå·²ç”Ÿæˆ: {REPORT_FILE}\n")
    print("å ±å‘Šå…§å®¹æ‘˜è¦:\n")
    print(report[:1000] + "\n...\n")


def main():
    """å‘½ä»¤åˆ—ä¸»ç¨‹å¼"""
    if len(sys.argv) < 2:
        print("""
ä»£ç†äººåˆ†æ´¾æ™ºæ…§åˆ†æå·¥å…·

ä½¿ç”¨æ–¹æ³•ï¼š
  python agent_dispatch_analytics.py analyze  - åˆ†ææ¨¡å¼
  python agent_dispatch_analytics.py suggest  - æ”¹é€²å»ºè­°
  python agent_dispatch_analytics.py trends   - è¶¨å‹¢è¿½è¹¤
  python agent_dispatch_analytics.py report   - å®Œæ•´å ±å‘Š
""")
        return

    command = sys.argv[1]

    if command == "analyze":
        cmd_analyze()
    elif command == "suggest":
        cmd_suggest()
    elif command == "trends":
        cmd_trends()
    elif command == "report":
        cmd_report()
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")


if __name__ == "__main__":
    main()
